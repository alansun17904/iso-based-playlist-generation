{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISO-Based Deep Learning Using LSTMs\n",
    "\n",
    "This notebook establishes the model architecture that is used to learn the mapping between various desired mood states into playlists. The notebook is split into four distinct section: *data loading*, *dataset*, *model architecture*, and finally *training and evaluation*. We proceed by summarizing these sections briefly; a detailed description of their purposes can be found under the section headers. \n",
    "\n",
    "The **data loading** section loads all of the variables from preprocessing, including the tokenization of the training set, as well as the tokenizer used to perform the tokenizations. \n",
    "\n",
    "The **dataset** section creates an `ISODataset` class which converts the Dataframe loaded in from the data loading section to be in a format which is easily accessible by torch. \n",
    "\n",
    "The **model architecture** section creates the actual model that is used for training. The specification of the model is also under its section heading. It should be important to note that `torch lightning` is used throughout the notebook, but in particular for designing the model architecture. Thus, the code in the training and evaluation section is minimal. This section also encapsulates the loss function and learning rate scheduler used for training.\n",
    "\n",
    "The **training and evaluation** section contains code which kickstarts the training of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Data from preprocessing is loaded into this notebook â€“ including the tokenizations, cleaned audio features, and the tokenizer itself. It is important to note that for ease of operation, we are pre-emptively removing all the rows in the data frame corresponding to empty playlists, as these are still WIP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', index_col=0)\n",
    "df = df[df['features'] != '[null]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_moods(self, moods):\n",
    "        flat = []\n",
    "        \n",
    "        Tokenizer.flatten(moods, flat)\n",
    "        vocab = sorted(set(flat))\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for index, word in enumerate(vocab):\n",
    "            self.stoi[word] = index\n",
    "        self.itos = {v : k for k, v in self.stoi.items()}\n",
    "\n",
    "    def flatten(l, flat):\n",
    "        \"\"\"\n",
    "        Recursively, flatten a list.\n",
    "        \"\"\"\n",
    "        if type(l) != list:\n",
    "            flat.append(l)\n",
    "        else:\n",
    "            for el in l:\n",
    "                Tokenizer.flatten(el, flat)\n",
    "\n",
    "    def moods_to_token(self, states, reverse=False):\n",
    "        \"\"\"\n",
    "        Recursively tokenize moods, while preserving the\n",
    "        structure of the list. When `reverse` is true, the\n",
    "        method translates the tokens back into the mood strings\n",
    "        \"\"\"\n",
    "        if type(states) != list:\n",
    "            if reverse:\n",
    "                return self.itos[states]\n",
    "            else:\n",
    "                return self.stoi[states]\n",
    "        else:\n",
    "            for index, state in enumerate(states):\n",
    "                states[index] = self.moods_to_token(state, reverse)\n",
    "            return states\n",
    "tokenizer = torch.load('tokenizer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this section, we package the training data into an `ISODataset` object. This is so that `torch`'s batching system can work with it more easily. Moreover, to make sure that all of the sequences are uniform, we assume that each states has at most 5 mood descriptors. Therefore, all the inputs to our network should be of shape `(batch_size, n, 5, 3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISODataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mood_states = json.loads(self.df.iloc[idx]['moods_states'])\n",
    "        for index, state in enumerate(mood_states):\n",
    "            mood_states[index] = np.pad(state, (0,5-len(state)), \n",
    "                                        constant_values=tokenizer.stoi['<pad>'])\n",
    "        mood_states = torch.LongTensor(mood_states)\n",
    "        \n",
    "        audio_features = self.df.iloc[idx]['features']\n",
    "        audio_features = torch.Tensor(json.loads(audio_features))\n",
    "        return mood_states, audio_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mood states are not only of variable length but also of variable dimension, we need to pad each batch so that a network will be able to process them. This preprocessing before it reaches the neural network is done through the `iso_collate` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_collate(batch):\n",
    "    moods, features = batch\n",
    "    lengths = []\n",
    "    for data_point in batch:\n",
    "        lengths.append(len(data_point))\n",
    "    moods = pad_sequence(moods, batch_first=True, padding_value=tokenizer.stoi['<pad>'])\n",
    "    return moods, torch.LongTensor(lengths), features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, tokenizer, decoder_dim=30, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding = nn.Embedding(len(self.tokenizer.itos),\n",
    "                                      3, padding_idx=self.tokenizer.stoi['<pad>'])\n",
    "        self.lstm = nn.LSTM(batch_first=True)\n",
    "    \n",
    "    def init_hidden_state(self, )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mood_states, audio_features = x\n",
    "        return self.embedding(mood_states)\n",
    "    \n",
    "    def step(self, batch, batch_idx):\n",
    "        return\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = ISODataset(df)\n",
    "a1 = nn.Embedding(len(tokenizer.itos), 3,\n",
    "                 padding_idx=tokenizer.stoi['<pad>'])\n",
    "tokens, labels = iso[0]\n",
    "b = a1(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8458, -0.9801, -0.2898],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [-1.1824,  0.1617,  0.0662],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [-0.1663,  0.1596, -0.9348],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.view(1, -1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5, 3])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = b.reshape((1, 3, 5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = nn.Linear(3, 2)\n",
    "a3 = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21 31 31 31 31]\n",
      "[ 3 31 31 31 31]\n",
      "[13 31 31 31 31]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5340, -0.2316, -0.6829],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 1.0501,  0.4073, -0.4818],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 1.3359, -0.3053, -1.3831],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Model(tokenizer)\n",
    "iso = ISODataset(df)\n",
    "m(iso[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[21, 31, 31, 31, 31],\n",
       "         [ 3, 31, 31, 31, 31],\n",
       "         [13, 31, 31, 31, 31]]),\n",
       " tensor([ 3, 10]),\n",
       " tensor([[ 9.6900e-01,  3.8800e-01,  8.5900e-02,  7.3500e-05,  7.0000e+00,\n",
       "           1.0800e-01, -1.6061e+01,  0.0000e+00,  4.7200e-02,  8.8253e+01,\n",
       "           1.9000e-01],\n",
       "         [ 9.7800e-01,  3.6700e-01,  1.1100e-01,  3.9700e-05,  4.0000e+00,\n",
       "           8.9700e-02, -1.4084e+01,  1.0000e+00,  9.7200e-02,  8.2642e+01,\n",
       "           1.9800e-01],\n",
       "         [ 9.6600e-01,  5.1300e-01,  1.8300e-01,  0.0000e+00,  2.0000e+00,\n",
       "           3.2700e-01, -1.0249e+01,  1.0000e+00,  4.0700e-02,  1.0718e+02,\n",
       "           2.6600e-01],\n",
       "         [ 3.9800e-01,  6.6800e-01,  5.4700e-01,  7.6600e-02,  1.0000e+00,\n",
       "           9.3100e-02, -8.0240e+00,  1.0000e+00,  3.5300e-02,  8.3500e+01,\n",
       "           1.9200e-01],\n",
       "         [ 8.5300e-01,  7.9400e-01,  3.2000e-01,  1.3400e-01,  1.0000e+00,\n",
       "           1.1200e-01, -1.2920e+01,  0.0000e+00,  1.7300e-01,  1.7409e+02,\n",
       "           2.4100e-01],\n",
       "         [ 8.6300e-02,  8.0100e-01,  5.8900e-01,  0.0000e+00,  7.0000e+00,\n",
       "           1.6200e-01, -5.1570e+00,  1.0000e+00,  4.8500e-02,  1.1693e+02,\n",
       "           5.8000e-01],\n",
       "         [ 1.9600e-01,  6.7300e-01,  7.0400e-01,  0.0000e+00,  6.0000e+00,\n",
       "           8.9800e-02, -8.0560e+00,  1.0000e+00,  3.6000e-01,  8.6919e+01,\n",
       "           3.7200e-01],\n",
       "         [ 1.2500e-01,  7.0800e-01,  6.3700e-01,  0.0000e+00,  4.0000e+00,\n",
       "           1.2900e-01, -5.7950e+00,  1.0000e+00,  8.3300e-02,  1.0412e+02,\n",
       "           8.6200e-01],\n",
       "         [ 5.0200e-03,  5.5100e-01,  8.3600e-01,  2.1000e-05,  1.0000e+01,\n",
       "           4.2500e-02, -3.8380e+00,  0.0000e+00,  5.2400e-02,  1.8506e+02,\n",
       "           7.5800e-01],\n",
       "         [ 6.1500e-02,  6.4700e-01,  8.5300e-01,  0.0000e+00,  8.0000e+00,\n",
       "           6.2000e-02, -4.5160e+00,  1.0000e+00,  9.0400e-02,  1.0385e+02,\n",
       "           6.1800e-01]]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iso_collate(iso[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[21, 31, 31, 31, 31],\n",
       "         [ 3, 31, 31, 31, 31],\n",
       "         [13, 31, 31, 31, 31]]),\n",
       " tensor([ 3, 10]),\n",
       " tensor([[ 9.6900e-01,  3.8800e-01,  8.5900e-02,  7.3500e-05,  7.0000e+00,\n",
       "           1.0800e-01, -1.6061e+01,  0.0000e+00,  4.7200e-02,  8.8253e+01,\n",
       "           1.9000e-01],\n",
       "         [ 9.7800e-01,  3.6700e-01,  1.1100e-01,  3.9700e-05,  4.0000e+00,\n",
       "           8.9700e-02, -1.4084e+01,  1.0000e+00,  9.7200e-02,  8.2642e+01,\n",
       "           1.9800e-01],\n",
       "         [ 9.6600e-01,  5.1300e-01,  1.8300e-01,  0.0000e+00,  2.0000e+00,\n",
       "           3.2700e-01, -1.0249e+01,  1.0000e+00,  4.0700e-02,  1.0718e+02,\n",
       "           2.6600e-01],\n",
       "         [ 3.9800e-01,  6.6800e-01,  5.4700e-01,  7.6600e-02,  1.0000e+00,\n",
       "           9.3100e-02, -8.0240e+00,  1.0000e+00,  3.5300e-02,  8.3500e+01,\n",
       "           1.9200e-01],\n",
       "         [ 8.5300e-01,  7.9400e-01,  3.2000e-01,  1.3400e-01,  1.0000e+00,\n",
       "           1.1200e-01, -1.2920e+01,  0.0000e+00,  1.7300e-01,  1.7409e+02,\n",
       "           2.4100e-01],\n",
       "         [ 8.6300e-02,  8.0100e-01,  5.8900e-01,  0.0000e+00,  7.0000e+00,\n",
       "           1.6200e-01, -5.1570e+00,  1.0000e+00,  4.8500e-02,  1.1693e+02,\n",
       "           5.8000e-01],\n",
       "         [ 1.9600e-01,  6.7300e-01,  7.0400e-01,  0.0000e+00,  6.0000e+00,\n",
       "           8.9800e-02, -8.0560e+00,  1.0000e+00,  3.6000e-01,  8.6919e+01,\n",
       "           3.7200e-01],\n",
       "         [ 1.2500e-01,  7.0800e-01,  6.3700e-01,  0.0000e+00,  4.0000e+00,\n",
       "           1.2900e-01, -5.7950e+00,  1.0000e+00,  8.3300e-02,  1.0412e+02,\n",
       "           8.6200e-01],\n",
       "         [ 5.0200e-03,  5.5100e-01,  8.3600e-01,  2.1000e-05,  1.0000e+01,\n",
       "           4.2500e-02, -3.8380e+00,  0.0000e+00,  5.2400e-02,  1.8506e+02,\n",
       "           7.5800e-01],\n",
       "         [ 6.1500e-02,  6.4700e-01,  8.5300e-01,  0.0000e+00,  8.0000e+00,\n",
       "           6.2000e-02, -4.5160e+00,  1.0000e+00,  9.0400e-02,  1.0385e+02,\n",
       "           6.1800e-01]]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iso_collate(iso[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
