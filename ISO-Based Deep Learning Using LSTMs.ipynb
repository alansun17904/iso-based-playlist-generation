{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISO-Based Deep Learning Using LSTMs\n",
    "\n",
    "This notebook establishes the model architecture that is used to learn the mapping between various desired mood states into playlists. The notebook is split into four distinct section: *data loading*, *dataset*, *model architecture*, and finally *training and evaluation*. We proceed by summarizing these sections briefly; a detailed description of their purposes can be found under the section headers. \n",
    "\n",
    "The **data loading** section loads all of the variables from preprocessing, including the tokenization of the training set, as well as the tokenizer used to perform the tokenizations. \n",
    "\n",
    "The **dataset** section creates an `ISODataset` class which converts the Dataframe loaded in from the data loading section to be in a format which is easily accessible by torch. \n",
    "\n",
    "The **model architecture** section creates the actual model that is used for training. The specification of the model is also under its section heading. It should be important to note that `torch lightning` is used throughout the notebook, but in particular for designing the model architecture. Thus, the code in the training and evaluation section is minimal. This section also encapsulates the loss function and learning rate scheduler used for training.\n",
    "\n",
    "The **training and evaluation** section contains code which kickstarts the training of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Data from preprocessing is loaded into this notebook – including the tokenizations, cleaned audio features, and the tokenizer itself. It is important to note that for ease of operation, we are pre-emptively removing all the rows in the data frame corresponding to empty playlists, as these are still WIP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', index_col=0)\n",
    "df = df[df['features'] != '[null]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_moods(self, moods):\n",
    "        flat = []\n",
    "        \n",
    "        Tokenizer.flatten(moods, flat)\n",
    "        vocab = sorted(set(flat))\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for index, word in enumerate(vocab):\n",
    "            self.stoi[word] = index\n",
    "        self.itos = {v : k for k, v in self.stoi.items()}\n",
    "\n",
    "    def flatten(l, flat):\n",
    "        \"\"\"\n",
    "        Recursively, flatten a list.\n",
    "        \"\"\"\n",
    "        if type(l) != list:\n",
    "            flat.append(l)\n",
    "        else:\n",
    "            for el in l:\n",
    "                Tokenizer.flatten(el, flat)\n",
    "\n",
    "    def moods_to_token(self, states, reverse=False):\n",
    "        \"\"\"\n",
    "        Recursively tokenize moods, while preserving the\n",
    "        structure of the list. When `reverse` is true, the\n",
    "        method translates the tokens back into the mood strings\n",
    "        \"\"\"\n",
    "        if type(states) != list:\n",
    "            if reverse:\n",
    "                return self.itos[states]\n",
    "            else:\n",
    "                return self.stoi[states]\n",
    "        else:\n",
    "            for index, state in enumerate(states):\n",
    "                states[index] = self.moods_to_token(state, reverse)\n",
    "            return states\n",
    "tokenizer = torch.load('tokenizer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this section, we package the training data into an `ISODataset` object. This is so that `torch`'s batching system can work with it more easily. Moreover, to make sure that all of the sequences are uniform, we assume that each states has at most 5 mood descriptors. Therefore, all the inputs to our network should be of shape `(batch_size, n, 5, 3)`, where $n$ is pre-determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISODataset(Dataset):\n",
    "    \"\"\"\n",
    "    The `ISODataset` class packages training data into a single index-able object.\n",
    "    This makes it easy for torch to use as a generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, maxlen=5):\n",
    "        \"\"\"\n",
    "        Initializer.\n",
    "        :param maxlen: The reader should note that this is the maximum number\n",
    "        of mood transitions there can be. The constants (5) proceeding this\n",
    "        block represent the number of descriptors allowed for each mood state.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.maxlen = maxlen\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        mood_states = json.loads(self.df.iloc[idx]['moods_states'])\n",
    "        for index, state in enumerate(mood_states):\n",
    "            mood_states[index] = np.pad(state, (0,5-len(state)), \n",
    "                                        constant_values=tokenizer.stoi['<pad>'])\n",
    "        while len(mood_states) < 5:\n",
    "            mood_states.append(np.full(5, tokenizer.stoi['<pad>']))\n",
    "        mood_states = torch.LongTensor(mood_states)\n",
    "        \n",
    "        audio_features = self.df.iloc[idx]['features']\n",
    "        audio_features = torch.Tensor(json.loads(audio_features))\n",
    "        return mood_states, audio_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mood states are not only of variable length but also of variable dimension, we need to pad each batch so that a network will be able to process them. This preprocessing before it reaches the neural network is done through the `iso_collate` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_collate(batch):\n",
    "    moods, features, lengths = [], [], []\n",
    "    for data_point in batch:\n",
    "        moods.append(data_point[0])\n",
    "        features.append(data_point[1])\n",
    "        lengths.append(len(data_point[1]))\n",
    "    moods = pad_sequence(moods, batch_first=True, padding_value=tokenizer.stoi['<pad>'])\n",
    "    return moods, torch.LongTensor(lengths), torch.stack(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    The attention mechanism of the network. On each time step, of the LSTM,\n",
    "    the LSTM cell looks at the previous hidden state as well as the input\n",
    "    to the LSTM, then it weights the various dimensions of the input based\n",
    "    on the hidden state / input. This is done by applying two linear on the\n",
    "    hidden and input states respectively, then combining the outputs, running\n",
    "    them through another linear layer, and interpolating the final weights \n",
    "    using the softmax function. The result of the attention layer, is a \n",
    "    sum product of all the weights and the respective attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, attention_dim=10, maxlen=5):\n",
    "        \"\"\"\n",
    "        Initializer for the attention mechanism of the network.\n",
    "        :embed_dim: the dimension of the embeddings – hyperparamters.\n",
    "        :param attention_dim: specifies dimension of the hidden attention\n",
    "        layer. This is simply a hyperparameter and will only affect the\n",
    "        efficacy of the network, not its functionality. \n",
    "        :maxlen: specifies the maximum number of mood transitions allowed.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.mood_attention = nn.Linear(self.embed_dim, self.attention_dim)\n",
    "        self.hidden_attention = nn.Linear(11, self.attention_dim)\n",
    "        # the input to the hidden attention is 11 as that is the size of the\n",
    "        # desired output dimension.\n",
    "        self.attention = nn.Linear(self.attention_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, moods, hidden):\n",
    "        \"\"\"\n",
    "        :param moods: The raw input mood states – this is of size (bs x maxlen x 5 x embed_dim).\n",
    "        The reader should note that this *needs* to be preprocessed into the size\n",
    "        (bs x (maxlen * 5) x embed_dim). \n",
    "        :param hidden: The previous hidden state of the LSTM cell – this should be of\n",
    "        size (bs x 10).\n",
    "        The result of this function is to find a weighting, or alternatively, where to \n",
    "        pay \"attention\" to based on the `moods` and `hidden` state. The weights\n",
    "        of the attention, `alpha` of size (bs x (maxlen * 5)), is the used to in a sum product\n",
    "        with the moods (bs x (maxlen * 5) x embed_dim), yielding a size of (bs x embed_dim).\n",
    "        This single vector then acts as the inputs to the LSTM cell. \n",
    "        \"\"\"\n",
    "        att1 = self.mood_attention(moods)\n",
    "        att2 = self.hidden_attention(hidden)\n",
    "        att = self.attention(self.relu(att1 + att2.unsqueeze(1)))\n",
    "        alpha = self.softmax(att)\n",
    "        weighted_moods = (moods * alpha).sum(dim=1)\n",
    "        return weighted_moods, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, tokenizer, decoder_dim=30, \n",
    "                 dropout=0.5, maxlen=5, embed_dim=3):\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embedding = nn.Embedding(len(self.tokenizer.itos), self.embed_dim, \n",
    "                                      padding_idx=self.tokenizer.stoi['<pad>'])\n",
    "        self.attention = Attention(self.embed_dim)\n",
    "        self.h0 = nn.Linear(maxlen * self.embed_dim * 5, 11)\n",
    "        self.c0 = nn.Linear(maxlen * self.embed_dim * 5, 11)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTMCell(self.embed_dim, 11)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.forget = nn.Linear(11, self.embed_dim)\n",
    "        self.fc = nn.Linear(11, 11)\n",
    "    def init_hidden_states(self, x):\n",
    "        return self.h0(x), self.c0(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mood_states, lengths, audio_features = x\n",
    "        bs = mood_states.size(0)\n",
    "        mood_states = self.embedding(mood_states)\n",
    "        moods = mood_states.view(bs, (self.maxlen * 5), self.embed_dim)\n",
    "        \n",
    "        sorted_lengths, indicies = lengths.sort(dim=0, descending=True)\n",
    "        moods, audio_features = moods[indicies], audio_features[indicies]\n",
    "        h, c = self.init_hidden_states(moods.view(bs, -1))  # (bs x 11)\n",
    "\n",
    "        predictions = torch.zeros(bs, max(lengths), 11)\n",
    "        for timestep in range(max(lengths)):\n",
    "            num_predict = sum([l > timestep for l in lengths])\n",
    "            attention_weighted_moods, alphas = self.attention(moods[:num_predict], \n",
    "                                                      h[:num_predict])\n",
    "            gate = self.sigmoid(self.forget(h[:num_predict]))\n",
    "            weighted_moods = gate * attention_weighted_moods\n",
    "            h, c = self.lstm(weighted_moods, \n",
    "                             (h[:num_predict], c[:num_predict]))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            \n",
    "            predictions[:num_predict, timestep, :] = preds\n",
    "        return predictions\n",
    "    \n",
    "    def step(self, batch, batch_idx):\n",
    "        return\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = ISODataset(df)\n",
    "train_loader = DataLoader(iso,\n",
    "                          batch_size=2,\n",
    "                          collate_fn=iso_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0300, -0.2939,  0.0356,  0.0756, -0.0758, -0.3103,  0.2413,\n",
      "           0.1079,  0.0090, -0.3218,  0.1812],\n",
      "         [-0.0710, -0.2483,  0.0902,  0.1239, -0.0228, -0.3764,  0.2613,\n",
      "           0.0640, -0.0089, -0.4363,  0.1493],\n",
      "         [-0.0501, -0.3078, -0.0362,  0.1607, -0.1655, -0.2557,  0.1498,\n",
      "           0.0883,  0.0369, -0.3876,  0.2003],\n",
      "         [ 0.0225, -0.2342,  0.0398,  0.0118, -0.2028, -0.2235,  0.2518,\n",
      "          -0.0930,  0.1303, -0.4472,  0.2662],\n",
      "         [-0.0281, -0.2407,  0.0054,  0.0630, -0.2936, -0.1781,  0.1455,\n",
      "          -0.0074,  0.1637, -0.3967,  0.2677],\n",
      "         [ 0.0710, -0.2479,  0.0310,  0.0236, -0.1314, -0.2743,  0.3431,\n",
      "           0.0909, -0.0655, -0.2804,  0.0971],\n",
      "         [ 0.0360, -0.2850, -0.0235,  0.0875, -0.1032, -0.2539,  0.2449,\n",
      "          -0.0686, -0.0120, -0.4144,  0.2340],\n",
      "         [ 0.0042, -0.2035,  0.1299,  0.0894, -0.1042, -0.3493,  0.2847,\n",
      "           0.0381,  0.0513, -0.4071,  0.2122],\n",
      "         [ 0.0514, -0.3355,  0.0153,  0.0388, -0.1696, -0.0760,  0.2690,\n",
      "          -0.0717,  0.1093, -0.3634,  0.1968],\n",
      "         [-0.0227, -0.2884, -0.0109,  0.1771, -0.1053, -0.2882,  0.1511,\n",
      "           0.0024,  0.0285, -0.4022,  0.2873]]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "for i in train_loader:\n",
    "    print(model(i)[:1])\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
