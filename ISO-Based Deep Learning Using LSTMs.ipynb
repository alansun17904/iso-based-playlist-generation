{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISO-Based Deep Learning Using LSTMs\n",
    "\n",
    "This notebook establishes the model architecture that is used to learn the mapping between various desired mood states into playlists. The notebook is split into four distinct section: *data loading*, *dataset*, *model architecture*, and finally *training and evaluation*. We proceed by summarizing these sections briefly; a detailed description of their purposes can be found under the section headers. \n",
    "\n",
    "The **data loading** section loads all of the variables from preprocessing, including the tokenization of the training set, as well as the tokenizer used to perform the tokenizations. \n",
    "\n",
    "The **dataset** section creates an `ISODataset` class which converts the Dataframe loaded in from the data loading section to be in a format which is easily accessible by torch. \n",
    "\n",
    "The **model architecture** section creates the actual model that is used for training. The specification of the model is also under its section heading. It should be important to note that `torch lightning` is used throughout the notebook, but in particular for designing the model architecture. Thus, the code in the training and evaluation section is minimal. This section also encapsulates the loss function and learning rate scheduler used for training.\n",
    "\n",
    "The **training and evaluation** section contains code which kickstarts the training of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import requests\n",
    "import datetime\n",
    "import collections\n",
    "import threading\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from scipy.special import softmax\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "\n",
    "Data from preprocessing is loaded into this notebook – including the tokenizations, cleaned audio features, and the tokenizer itself. It is important to note that for ease of operation, we are pre-emptively removing all the rows in the data frame corresponding to empty playlists, as these are still WIP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv', index_col=0)\n",
    "df = df[df['features'] != '[null]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moods_states</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>iso26</td>\n",
       "      <td>[[22], [20]]</td>\n",
       "      <td>[[0.755, 0.479, 0.154, -15.051, 0.0369, 0.232]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso23</td>\n",
       "      <td>[[28], [20]]</td>\n",
       "      <td>[[0.455, 0.674, 0.615, -8.188, 0.147, 0.756], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso30</td>\n",
       "      <td>[[19, 1, 9], [6, 21, 25]]</td>\n",
       "      <td>[[0.907, 0.23, 0.159, -16.315, 0.0323, 0.039],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso19</td>\n",
       "      <td>[[17], [26], [0]]</td>\n",
       "      <td>[[0.985, 0.653, 0.178, -13.47, 0.0312, 0.225],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso22</td>\n",
       "      <td>[[28], [26]]</td>\n",
       "      <td>[[0.155, 0.221, 0.0879, -16.996, 0.0381, 0.040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso24</td>\n",
       "      <td>[[22], [11]]</td>\n",
       "      <td>[[0.975, 0.462, 0.203, -16.313, 0.0355, 0.437]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso18</td>\n",
       "      <td>[[13], [16], [25]]</td>\n",
       "      <td>[[0.124, 0.585, 0.52, -6.136, 0.0712, 0.129], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso25</td>\n",
       "      <td>[[22], [26]]</td>\n",
       "      <td>[[0.953, 0.582, 0.199, -10.045, 0.0321, 0.0783...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso17</td>\n",
       "      <td>[[23], [4], [9]]</td>\n",
       "      <td>[[0.755, 0.479, 0.154, -15.051, 0.0369, 0.232]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso20</td>\n",
       "      <td>[[5], [16], [21]]</td>\n",
       "      <td>[[0.948, 0.571, 0.0274, -20.274, 0.0649, 0.087...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso15</td>\n",
       "      <td>[[27], [14], [9]]</td>\n",
       "      <td>[[0.982, 0.532, 0.137, -18.208, 0.0717, 0.301]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso14</td>\n",
       "      <td>[[9], [8], [21]]</td>\n",
       "      <td>[[0.262, 0.696, 0.686, -6.113, 0.0309, 0.7], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso11</td>\n",
       "      <td>[[22], [25], [29]]</td>\n",
       "      <td>[[0.92, 0.587, 0.229, -11.254, 0.128, 0.379], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso16</td>\n",
       "      <td>[[22], [4], [14]]</td>\n",
       "      <td>[[0.969, 0.388, 0.0859, -16.061, 0.0472, 0.19]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso21</td>\n",
       "      <td>[[28], [11]]</td>\n",
       "      <td>[[0.942, 0.252, 0.314, -18.197, 0.0407, 0.2], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso13</td>\n",
       "      <td>[[19], [4], [9]]</td>\n",
       "      <td>[[0.932, 0.433, 0.329, -13.288, 0.0343, 0.264]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso12</td>\n",
       "      <td>[[3], [2], [25]]</td>\n",
       "      <td>[[0.186, 0.548, 0.532, -7.596, 0.0323, 0.405],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso09</td>\n",
       "      <td>[[22], [11]]</td>\n",
       "      <td>[[0.778, 0.407, 0.308, -9.001, 0.0296, 0.153],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso05</td>\n",
       "      <td>[[15], [7]]</td>\n",
       "      <td>[[0.93, 0.628, 0.179, -16.179, 0.0328, 0.226],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso10</td>\n",
       "      <td>[[5], [21], [14]]</td>\n",
       "      <td>[[0.942, 0.252, 0.314, -18.197, 0.0407, 0.2], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso06</td>\n",
       "      <td>[[22], [12]]</td>\n",
       "      <td>[[0.301, 0.529, 0.381, -11.053, 0.0306, 0.0712...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso08</td>\n",
       "      <td>[[19], [6]]</td>\n",
       "      <td>[[0.251, 0.513, 0.767, -8.386, 0.0918, 0.429],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso07</td>\n",
       "      <td>[[24], [10]]</td>\n",
       "      <td>[[0.872, 0.455, 0.39, -11.886, 0.0771, 0.171],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso03</td>\n",
       "      <td>[[5], [6, 18]]</td>\n",
       "      <td>[[0.864, 0.0783, 0.206, -20.611, 0.0337, 0.080...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>iso04</td>\n",
       "      <td>[[6, 8, 17], [6, 21, 16]]</td>\n",
       "      <td>[[0.166, 0.282, 0.342, -12.263, 0.0314, 0.087]...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    moods_states  \\\n",
       "iso26               [[22], [20]]   \n",
       "iso23               [[28], [20]]   \n",
       "iso30  [[19, 1, 9], [6, 21, 25]]   \n",
       "iso19          [[17], [26], [0]]   \n",
       "iso22               [[28], [26]]   \n",
       "iso24               [[22], [11]]   \n",
       "iso18         [[13], [16], [25]]   \n",
       "iso25               [[22], [26]]   \n",
       "iso17           [[23], [4], [9]]   \n",
       "iso20          [[5], [16], [21]]   \n",
       "iso15          [[27], [14], [9]]   \n",
       "iso14           [[9], [8], [21]]   \n",
       "iso11         [[22], [25], [29]]   \n",
       "iso16          [[22], [4], [14]]   \n",
       "iso21               [[28], [11]]   \n",
       "iso13           [[19], [4], [9]]   \n",
       "iso12           [[3], [2], [25]]   \n",
       "iso09               [[22], [11]]   \n",
       "iso05                [[15], [7]]   \n",
       "iso10          [[5], [21], [14]]   \n",
       "iso06               [[22], [12]]   \n",
       "iso08                [[19], [6]]   \n",
       "iso07               [[24], [10]]   \n",
       "iso03             [[5], [6, 18]]   \n",
       "iso04  [[6, 8, 17], [6, 21, 16]]   \n",
       "\n",
       "                                                features  \n",
       "iso26  [[0.755, 0.479, 0.154, -15.051, 0.0369, 0.232]...  \n",
       "iso23  [[0.455, 0.674, 0.615, -8.188, 0.147, 0.756], ...  \n",
       "iso30  [[0.907, 0.23, 0.159, -16.315, 0.0323, 0.039],...  \n",
       "iso19  [[0.985, 0.653, 0.178, -13.47, 0.0312, 0.225],...  \n",
       "iso22  [[0.155, 0.221, 0.0879, -16.996, 0.0381, 0.040...  \n",
       "iso24  [[0.975, 0.462, 0.203, -16.313, 0.0355, 0.437]...  \n",
       "iso18  [[0.124, 0.585, 0.52, -6.136, 0.0712, 0.129], ...  \n",
       "iso25  [[0.953, 0.582, 0.199, -10.045, 0.0321, 0.0783...  \n",
       "iso17  [[0.755, 0.479, 0.154, -15.051, 0.0369, 0.232]...  \n",
       "iso20  [[0.948, 0.571, 0.0274, -20.274, 0.0649, 0.087...  \n",
       "iso15  [[0.982, 0.532, 0.137, -18.208, 0.0717, 0.301]...  \n",
       "iso14  [[0.262, 0.696, 0.686, -6.113, 0.0309, 0.7], [...  \n",
       "iso11  [[0.92, 0.587, 0.229, -11.254, 0.128, 0.379], ...  \n",
       "iso16  [[0.969, 0.388, 0.0859, -16.061, 0.0472, 0.19]...  \n",
       "iso21  [[0.942, 0.252, 0.314, -18.197, 0.0407, 0.2], ...  \n",
       "iso13  [[0.932, 0.433, 0.329, -13.288, 0.0343, 0.264]...  \n",
       "iso12  [[0.186, 0.548, 0.532, -7.596, 0.0323, 0.405],...  \n",
       "iso09  [[0.778, 0.407, 0.308, -9.001, 0.0296, 0.153],...  \n",
       "iso05  [[0.93, 0.628, 0.179, -16.179, 0.0328, 0.226],...  \n",
       "iso10  [[0.942, 0.252, 0.314, -18.197, 0.0407, 0.2], ...  \n",
       "iso06  [[0.301, 0.529, 0.381, -11.053, 0.0306, 0.0712...  \n",
       "iso08  [[0.251, 0.513, 0.767, -8.386, 0.0918, 0.429],...  \n",
       "iso07  [[0.872, 0.455, 0.39, -11.886, 0.0771, 0.171],...  \n",
       "iso03  [[0.864, 0.0783, 0.206, -20.611, 0.0337, 0.080...  \n",
       "iso04  [[0.166, 0.282, 0.342, -12.263, 0.0314, 0.087]...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stoi)\n",
    "    \n",
    "    def fit_on_moods(self, moods):\n",
    "        flat = []\n",
    "        \n",
    "        Tokenizer.flatten(moods, flat)\n",
    "        vocab = sorted(set(flat))\n",
    "        vocab.append('<sos>')\n",
    "        vocab.append('<eos>')\n",
    "        vocab.append('<pad>')\n",
    "        for index, word in enumerate(vocab):\n",
    "            self.stoi[word] = index\n",
    "        self.itos = {v : k for k, v in self.stoi.items()}\n",
    "\n",
    "    def flatten(l, flat):\n",
    "        \"\"\"\n",
    "        Recursively, flatten a list.\n",
    "        \"\"\"\n",
    "        if type(l) != list:\n",
    "            flat.append(l)\n",
    "        else:\n",
    "            for el in l:\n",
    "                Tokenizer.flatten(el, flat)\n",
    "\n",
    "    def moods_to_token(self, states, reverse=False):\n",
    "        \"\"\"\n",
    "        Recursively tokenize moods, while preserving the\n",
    "        structure of the list. When `reverse` is true, the\n",
    "        method translates the tokens back into the mood strings\n",
    "        \"\"\"\n",
    "        if type(states) != list:\n",
    "            if reverse:\n",
    "                return self.itos[states]\n",
    "            else:\n",
    "                return self.stoi[states]\n",
    "        else:\n",
    "            for index, state in enumerate(states):\n",
    "                states[index] = self.moods_to_token(state, reverse)\n",
    "            return states\n",
    "tokenizer = torch.load('data/tokenizer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "In this section, we package the training data into an `ISODataset` object. This is so that `torch`'s batching system can work with it more easily. Moreover, to make sure that all of the sequences are uniform, we assume that each states has at most 5 mood descriptors. Therefore, all the inputs to our network should be of shape `(batch_size, n, 5, 3)`, where $n$ is pre-determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "We implement two data augmentations:\n",
    "\n",
    "- Random protuberance (randomly shifting the audio features by a particular specified percentage)\n",
    "- Reversing mood states and audio features (self explanatory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    def __init__(self, transformations):\n",
    "        self.transform = transformations\n",
    "    \n",
    "    def __call__(self, moods, features):\n",
    "        for trans in self.transform:\n",
    "            moods, features = trans(moods, features)\n",
    "        return moods, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureProtuberance:\n",
    "    def __init__(self, max_protuberance, phi):\n",
    "        \"\"\"\n",
    "        :param max_protuberance: the maximum percentage of protuberance.\n",
    "        If 0.5 is given then each component, c, in the feature matrix \n",
    "        will have a potential new min/max of c +- 0.5 * c.\n",
    "        :param phi: the probability that a given component is going to\n",
    "        be augmented. \n",
    "        \"\"\"\n",
    "        self.protuberance = max_protuberance\n",
    "        self.phi = phi\n",
    "    \n",
    "    def __call__(self, moods, features):\n",
    "        pct = (torch.randn(features.size()) - 0.5)\n",
    "        pct = pct * self.phi\n",
    "        aug = torch.randn(features.size()) > self.phi\n",
    "        return moods, features + aug * pct * features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reverse:\n",
    "    def __init__(self, phi):\n",
    "        \"\"\"\n",
    "        :param phi: (0, 1), the probability that the mood states and \n",
    "        features will be reversed.\n",
    "        \"\"\"\n",
    "        self.phi = phi\n",
    "        \n",
    "    def __call__(self, moods, features):\n",
    "        if random.random() > self.phi:\n",
    "            return moods, features\n",
    "        return (torch.flip(moods, dims=(0,)),\n",
    "                torch.flip(features, dims=(0,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISODataset(Dataset):\n",
    "    \"\"\"\n",
    "    The `ISODataset` class packages training data into a single index-able object.\n",
    "    This makes it easy for torch to use as a generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, maxlen=5, transform=None, batch_size=0):\n",
    "        \"\"\"\n",
    "        Initializer.\n",
    "        :param maxlen: The reader should note that this is the maximum number\n",
    "        of mood transitions there can be. The constants (5) proceeding this\n",
    "        block represent the number of descriptors allowed for each mood state.\n",
    "        \"\"\"\n",
    "        self.pca = None\n",
    "        self.n_comp = 11\n",
    "        self.components = np.array([])\n",
    "        self.mean = np.array([])\n",
    "        self.df = df\n",
    "        self.maxlen = maxlen\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "    \n",
    "    def pca_reduction(self, percent_var=0.95):\n",
    "        \"\"\"\n",
    "        Reduce the dimensionality of the data from 11->n. Where\n",
    "        the sum of the average percentage variances calculated by\n",
    "        eig / tr(D), where D is the diagonal matrix of eigenvalues\n",
    "        is greater than `percent_var`. It is important to note that \n",
    "        we assume a dataframe where the initial values are still in json\n",
    "        form. \n",
    "        :param var: must be greater than 0 and less than 1. \n",
    "        The method then uses these eigenvectors to reduce the dimensions\n",
    "        of the audio features, and returns the number of components being\n",
    "        used as well as the matrix of eigenvectors. \n",
    "        \"\"\"\n",
    "        # We proceed by stacking all the songs into a large matrix\n",
    "        all_playlists = [json.loads(self.df.iloc[entry]['features'])\n",
    "                         for entry in range(len(self.df))]\n",
    "        all_playlists = np.vstack(all_playlists)\n",
    "        self.mean = torch.mean(torch.from_numpy(all_playlists), 0)\n",
    "        self.pca = decomposition.PCA()\n",
    "        self.pca.fit(all_playlists)\n",
    "        \n",
    "        p_singular = self.pca.singular_values_ / sum(self.pca.singular_values_)\n",
    "        counter, i = 0, 0\n",
    "        while i < percent_var:\n",
    "            i += p_singular[counter]\n",
    "            counter += 1\n",
    "        self.n_comp = counter\n",
    "\n",
    "        self.components = torch.from_numpy(self.pca.components_[:self.n_comp,:]).float()\n",
    "        return self.pca.components_[:counter,:], counter\n",
    "        \n",
    "    def pca_reconstruction(self, y):\n",
    "        \"\"\"\n",
    "        This method converts a given matrix of predicted `y` datapoints\n",
    "        which are in reduced PCA form back into their `full` features \n",
    "        using PCA reconstruction. This is done by simply right-multiplying\n",
    "        `y` by our eigenvectors, and the adding the mean.\n",
    "        \"\"\"\n",
    "        Xhat = torch.matmul(y, self.components)\n",
    "        return Xhat + self.mean.float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(len(self.df), self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.df)\n",
    "        mood_states = json.loads(self.df.iloc[idx]['moods_states'])\n",
    "        audio_features = self.df.iloc[idx]['features']\n",
    "        audio_features = torch.Tensor(json.loads(audio_features))\n",
    "        for index, state in enumerate(mood_states):\n",
    "            mood_states[index] = np.pad(state, (0,5-len(state)), \n",
    "                                        constant_values=tokenizer.stoi['<pad>'])\n",
    "        while len(mood_states) < 5:\n",
    "            mood_states.append(np.full(5, tokenizer.stoi['<pad>']))\n",
    "        mood_states = torch.LongTensor(mood_states)\n",
    "        \n",
    "        # augmentations\n",
    "        if self.transform:\n",
    "            return self.transform(mood_states, audio_features)\n",
    "        return mood_states, audio_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = ISODataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The `ISODataset` class packages training data into a single index-able object.\n",
    "    This makes it easy for torch to use as a generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, maxlen=5, transform=None, batch_size=0):\n",
    "        \"\"\"\n",
    "        Initializer.\n",
    "        :param maxlen: The reader should note that this is the maximum number\n",
    "        of mood transitions there can be. The constants (5) proceeding this\n",
    "        block represent the number of descriptors allowed for each mood state.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.maxlen = maxlen\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return max(len(self.df), self.batch_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.df)\n",
    "        mood_states = json.loads(self.df.iloc[idx]['moods_states'])\n",
    "        for index, state in enumerate(mood_states):\n",
    "            mood_states[index] = np.pad(state, (0,5-len(state)), \n",
    "                                        constant_values=tokenizer.stoi['<pad>'])\n",
    "        while len(mood_states) < 5:\n",
    "            mood_states.append(np.full(5, tokenizer.stoi['<pad>']))\n",
    "        mood_states = torch.LongTensor(mood_states)\n",
    "        \n",
    "        return mood_states, self.df.iloc[idx]['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso = ISODataset(df.copy())\n",
    "iso.pca_reduction(percent_var=0.999999)\n",
    "eigs = iso.components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mood states are not only of variable length but also of variable dimension, we need to pad each batch so that a network will be able to process them. This preprocessing before it reaches the neural network is done through the `iso_collate` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iso_collate(batch):\n",
    "    moods, features, lengths = [], [], []\n",
    "    for data_point in batch:\n",
    "        moods.append(data_point[0])\n",
    "        features.append(data_point[1])\n",
    "        lengths.append(len(data_point[1]))\n",
    "    features = pad_sequence(features, batch_first=True)\n",
    "    moods = pad_sequence(moods, batch_first=True, padding_value=tokenizer.stoi['<pad>'])\n",
    "    return moods, torch.LongTensor(lengths), features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_collate(batch):\n",
    "    moods, lengths = [], []\n",
    "    for data_point in batch:\n",
    "        moods.append(data_point[0])\n",
    "        lengths.append(data_point[1])\n",
    "    moods = pad_sequence(moods, batch_first=True, padding_value=tokenizer.stoi['<pad>'])\n",
    "    return moods, torch.LongTensor(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n",
    "The model is split into two distinct components: the attention mechanism, and the LSTM component. The interaction between these two components are shown in the figure below:\n",
    "\n",
    "![ISONet](imgs/isonet-lstm-attention-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    The attention mechanism of the network. On each time step, of the LSTM,\n",
    "    the LSTM cell looks at the previous hidden state as well as the input\n",
    "    to the LSTM, then it weights the various dimensions of the input based\n",
    "    on the hidden state / input. This is done by applying two linear on the\n",
    "    hidden and input states respectively, then combining the outputs, running\n",
    "    them through another linear layer, and interpolating the final weights \n",
    "    using the softmax function. The result of the attention layer, is a \n",
    "    sum product of all the weights and the respective attributes.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, attention_dim=40, maxlen=5, output_dim=6):\n",
    "        \"\"\"\n",
    "        Initializer for the attention mechanism of the network.\n",
    "        :embed_dim: the dimension of the embeddings – hyperparamters.\n",
    "        :param attention_dim: specifies dimension of the hidden attention\n",
    "        layer. This is simply a hyperparameter and will only affect the\n",
    "        efficacy of the network, not its functionality. \n",
    "        :param maxlen: specifies the maximum number of mood transitions allowed.\n",
    "        :param output_dim: the dimension of the output, this varies as we \n",
    "        include/exclude prediction feature. We note that to predict all of the features,\n",
    "        we simply use the default value of 11.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.mood_attention1 = nn.Linear(self.embed_dim, self.attention_dim)\n",
    "        self.mood_attention2 = nn.Linear(self.attention_dim, self.attention_dim)\n",
    "        self.hidden_attention1 = nn.Linear(output_dim, self.attention_dim)\n",
    "        self.hidden_attention2 = nn.Linear(self.attention_dim, self.attention_dim)\n",
    "        # the input to the hidden attention is 11 as that is the size of the\n",
    "        # desired output dimension.\n",
    "        self.attention = nn.Linear(self.attention_dim, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, moods, hidden):\n",
    "        \"\"\"\n",
    "        :param moods: The raw input mood states – this is of size (bs x maxlen x 5 x embed_dim).\n",
    "        The reader should note that this *needs* to be preprocessed into the size\n",
    "        (bs x (maxlen * 5) x embed_dim). \n",
    "        :param hidden: The previous hidden state of the LSTM cell – this should be of\n",
    "        size (bs x 10).\n",
    "        The result of this function is to find a weighting, or alternatively, where to \n",
    "        pay \"attention\" to based on the `moods` and `hidden` state. The weights\n",
    "        of the attention, `alpha` of size (bs x (maxlen * 5)), is the used to in a sum product\n",
    "        with the moods (bs x (maxlen * 5) x embed_dim), yielding a size of (bs x embed_dim).\n",
    "        This single vector then acts as the inputs to the LSTM cell. \n",
    "        \"\"\"\n",
    "        att1 = self.relu(self.mood_attention1(moods))\n",
    "        att1 = self.mood_attention2(att1)\n",
    "        att2 = self.relu(self.hidden_attention1(hidden))\n",
    "        att2 = self.hidden_attention2(att2)\n",
    "        att = self.attention(self.relu(att1 + att2.unsqueeze(1)))\n",
    "        alpha = self.softmax(att)\n",
    "        weighted_moods = (moods * alpha).sum(dim=1)\n",
    "        return weighted_moods, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    An attention-based, one-directional baseline model.\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer=tokenizer, dropout=0.0, maxlen=5, embed_dim=3, lr=1e-2,\n",
    "                 weight_decay=1e-9, hidden_dim=6, output_dim=6,\n",
    "                 dataset=None):\n",
    "        \"\"\"\n",
    "        Initializer.\n",
    "        :param tokenizer: the tokenizer used to create the tokenizations for\n",
    "        the mood states and descriptors. \n",
    "        :param dropout: the probability of dropout of the layer between the \n",
    "        hidden state and the final output of each LSTM cell.\n",
    "        :param maxlen: the maximum number of mood transition states that are allowed. \n",
    "        It should be noted that this must be greater than any of the number of the \n",
    "        states associated with each datapoint; otherwise, it will cause errors. \n",
    "        :param embed_dim: the dimensionality of each embedding.\n",
    "        :param lr: learning rate of the network, TODO: implement separate learning rates\n",
    "        for the attention network and the LSTM cell.\n",
    "        :param output_dim: the dimension of the output, this varies as we \n",
    "        include/exclude prediction feature. We note that to predict all of the features,\n",
    "        we simply use the default value of 11.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.bce = nn.BCELoss(reduction='none')\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.weight_decay = weight_decay\n",
    "        self.maxlen = maxlen\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dataset = dataset\n",
    "        self.embedding = nn.Embedding(len(self.tokenizer.itos), self.embed_dim, \n",
    "                                      padding_idx=self.tokenizer.stoi['<pad>'])\n",
    "        self.attention = Attention(self.embed_dim, output_dim=hidden_dim)\n",
    "        self.h0 = nn.Linear(maxlen * self.embed_dim * 5, \n",
    "                            self.hidden_dim)\n",
    "        self.c0 = nn.Linear(maxlen * self.embed_dim * 5,\n",
    "                            self.hidden_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.lstm = nn.LSTMCell(self.embed_dim, self.hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.forget = nn.Linear(self.hidden_dim, self.embed_dim)\n",
    "        self.fc = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    def init_hidden_states(self, x):\n",
    "        \"\"\"\n",
    "        Given the mood states: flattened into a (bs x (maxlen * 5 * 3)) vector,\n",
    "        we use two separate linear layers to find the initial cell state and \n",
    "        initial hidden state. This is necessary over simply random initialization,\n",
    "        as the attention associated with the first cell is dependnet on h0.\n",
    "        \"\"\"\n",
    "        return self.h0(x), self.c0(x)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward feeding of the model. The network proceeds by first converting the mood\n",
    "        states into their respective embedding representations. Then, the flattened inputs\n",
    "        are used to determine the initial hidden/cell states of the given LSTM. The given\n",
    "        inputs are then sorted based on the length of their outputs. This makes it easier\n",
    "        for prediction. \n",
    "\n",
    "        :param x: a tuple that contains three items, the first being the various\n",
    "        mood states that are being queried, the second being the lenghts of the\n",
    "        desired labels of each datapoint, and finally the features – the target \n",
    "        outputs. \n",
    "        \"\"\"\n",
    "        mood_states, lengths, audio_features = x\n",
    "        bs = mood_states.size(0)\n",
    "        mood_states = self.embedding(mood_states)\n",
    "        moods = mood_states.view(bs, (self.maxlen * 5), self.embed_dim)\n",
    "        \n",
    "        sorted_lengths, indicies = lengths.sort(dim=0, descending=True)\n",
    "        moods, audio_features = moods[indicies], audio_features[indicies]\n",
    "        h, c = self.init_hidden_states(moods.view(bs, -1))  # (bs x output_dim)\n",
    "\n",
    "        predictions = torch.zeros(bs, max(lengths), self.output_dim)\n",
    "        for timestep in range(max(lengths)):\n",
    "            num_predict = sum([l > timestep for l in lengths])\n",
    "            attention_weighted_moods, alphas = self.attention(moods[:num_predict], \n",
    "                                                      h[:num_predict])\n",
    "            gate = self.sigmoid(self.forget(h[:num_predict]))\n",
    "            weighted_moods = gate * attention_weighted_moods\n",
    "            h, c = self.lstm(weighted_moods, \n",
    "                             (h[:num_predict], c[:num_predict]))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            \n",
    "            predictions[:num_predict, timestep, :] = preds\n",
    "        return self.sigmoid_relevant(predictions), audio_features\n",
    "    \n",
    "    def sigmoid_relevant(self, predictions):\n",
    "        # entropy loss for attributes ['danceability' (0), 'energy' (1), x'loudness' (2), \n",
    "        #           'speechiness' (3), 'acousticness' (4),\n",
    "        #           'valence' (5)]\n",
    "        loss = 0\n",
    "        for attr in [0, 1, 3, 4, 5]:\n",
    "            predictions[:,:,attr] = F.sigmoid(predictions[:,:,attr])\n",
    "        return predictions\n",
    "    \n",
    "    def step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        One \"step\" of the model. \n",
    "        \"\"\"\n",
    "        predictions, targets = self(batch)\n",
    "        if self.dataset is not None:\n",
    "            predictions = self.dataset.pca_reconstruction(predictions)\n",
    "        loss = self.entropy_loss(predictions, targets)\n",
    "        return loss, {'loss': loss}\n",
    "\n",
    "    def entropy_loss(self, predictions, targets):\n",
    "        # entropy loss for attributes ['danceability' (0), 'energy' (1), x'loudness' (2), \n",
    "        #           'speechiness' (3), 'acousticness' (4),\n",
    "        #           'valence' (5)]\n",
    "        loss = 0\n",
    "        for attr in [0, 1, 3, 4, 5]:\n",
    "            attr_loss = self.bce(predictions[:,:,attr], targets[:,:,attr])\n",
    "            loss += abs(attr_loss).sum(axis=1).mean()\n",
    "        # mse loss for attributes (loudness 2)\n",
    "        for attr in [2]:\n",
    "            attr_loss = self.mse(predictions[:,:,attr], targets[:,:,attr])\n",
    "            loss += attr_loss.sum(axis=1).mean()\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, logs = self.step(batch, batch_idx)\n",
    "        self.log_dict({f'train_{k}': v for k, v in logs.items()},\n",
    "                      on_step=True, on_epoch=True, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, logs = self.step(batch, batch_idx)\n",
    "        self.log_dict({f'val_{k}': v for k, v in logs.items()}, sync_dist=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        mood_states, lengths = batch\n",
    "        bs = mood_states.size(0)\n",
    "        mood_states = self.embedding(mood_states)\n",
    "        moods = mood_states.view(bs, (self.maxlen * 5), self.embed_dim)\n",
    "        \n",
    "        sorted_lengths, indicies = lengths.sort(dim=0, descending=True)\n",
    "        moods = moods[indicies]\n",
    "        h, c = self.init_hidden_states(moods.view(bs, -1))  # (bs x output_dim)\n",
    "\n",
    "        predictions = torch.zeros(bs, max(lengths), self.output_dim)\n",
    "        for timestep in range(max(lengths)):\n",
    "            num_predict = sum([l > timestep for l in lengths])\n",
    "            attention_weighted_moods, alphas = self.attention(moods[:num_predict], \n",
    "                                                      h[:num_predict])\n",
    "            gate = self.sigmoid(self.forget(h[:num_predict]))\n",
    "            weighted_moods = gate * attention_weighted_moods\n",
    "            h, c = self.lstm(weighted_moods, \n",
    "                             (h[:num_predict], c[:num_predict]))\n",
    "            preds = self.fc(self.dropout(h))\n",
    "            \n",
    "            predictions[:num_predict, timestep, :] = preds\n",
    "        return predictions\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configuration of the optimizer used to train the model.\n",
    "        This method is implicitly called by torch lightning during training. \n",
    "        Note that the learning rate and weight decay is given by the initialization\n",
    "        parameters of the model. \n",
    "        \"\"\"\n",
    "        return (optim.Adam(self.parameters(), lr=self.lr,\n",
    "                         weight_decay=self.weight_decay))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n",
    "This section is simplified by the torch lightning interface. Parameters involving training including epochs as well as checkpoints are paramterized in the initialization of `Trainer` object. We note here that there is no validation set as there are too little data points. Therefore, all the data is being used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'affectionate',\n",
       " 1: 'agitated',\n",
       " 2: 'amused',\n",
       " 3: 'angry',\n",
       " 4: 'animated',\n",
       " 5: 'anxious',\n",
       " 6: 'calm',\n",
       " 7: 'connected',\n",
       " 8: 'dreamy',\n",
       " 9: 'energetic',\n",
       " 10: 'energized',\n",
       " 11: 'happy',\n",
       " 12: 'hopeful',\n",
       " 13: 'irritated',\n",
       " 14: 'joyful',\n",
       " 15: 'lonely',\n",
       " 16: 'meditative',\n",
       " 17: 'melancholic',\n",
       " 18: 'motivated',\n",
       " 19: 'nervous',\n",
       " 20: 'powerful',\n",
       " 21: 'relaxed',\n",
       " 22: 'sad',\n",
       " 23: 'serene',\n",
       " 24: 'sluggish',\n",
       " 25: 'soothed',\n",
       " 26: 'tender',\n",
       " 27: 'tenderness',\n",
       " 28: 'tense',\n",
       " 29: 'triumphant',\n",
       " 30: '<sos>',\n",
       " 31: '<eos>',\n",
       " 32: '<pad>'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "#     FeatureProtuberance(0.10, 0.5),\n",
    "    Reverse(0.3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 64\n",
    "STEPS_P_EPOCH = 2000\n",
    "EPOCHS = 150 \n",
    "\n",
    "iso = ISODataset(df.copy(), transform=transform,\n",
    "                 batch_size=STEPS_P_EPOCH)\n",
    "# eigs, out_dim = iso.pca_reduction(percent_var=0.98)\n",
    "train_loader = DataLoader(iso,\n",
    "                          batch_size=BATCHSIZE,\n",
    "                          collate_fn=iso_collate)\n",
    "model = Model(tokenizer, embed_dim=64, hidden_dim=256, \n",
    "              dropout=0.1, lr=1e-3, weight_decay=1e-9) # , output_dim=out_dim,\n",
    "              # dataset=iso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: you defined a validation_step but have no val_dataloader. Skipping val loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "   | Name      | Type      | Params\n",
      "-----------------------------------------\n",
      "0  | bce       | BCELoss   | 0     \n",
      "1  | mse       | MSELoss   | 0     \n",
      "2  | embedding | Embedding | 2.1 K \n",
      "3  | attention | Attention | 16.2 K\n",
      "4  | h0        | Linear    | 409 K \n",
      "5  | c0        | Linear    | 409 K \n",
      "6  | dropout   | Dropout   | 0     \n",
      "7  | lstm      | LSTMCell  | 329 K \n",
      "8  | sigmoid   | Sigmoid   | 0     \n",
      "9  | forget    | Linear    | 16.4 K\n",
      "10 | fc        | Linear    | 1.5 K \n",
      "-----------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.743     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f1de51377d404292be95e597c32844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/opt/anaconda3/lib/python3.7/site-packages/pytorch_lightning/core/step_result.py:115: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  value = torch.tensor(value, device=device, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=EPOCHS)\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = datetime.datetime.today()\n",
    "torch.save(model.state_dict(), f'models/modified-loss-{n.month}-{n.day}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "We proceed to test our model through inference, just to see how well it will be able to perform. I note here that with Spotify, the model was able to generate a somewhat logical 10-song playlist for sad->happy. However, there were many songs that were repeated.\n",
    "\n",
    "I suspect this to be a result of having the same song/artist/genre as the seed genre/artist/song for all of the predictions. Therefore, in this next iteration, we may vary this between each song, and also add some random pertuberbance.\n",
    "\n",
    "Below are the playlists that were generated on the last iteration:\n",
    "```\n",
    "1  : ghostin by Ariana Grande\n",
    "2  : The Rose Song - From \"High School Musical: The Musical: The Series (Season 2)\" by Olivia Rodrigo\n",
    "3  : Lose You To Love Me by Selena Gomez\n",
    "4  : Cherry Wine - Live by Hozier\n",
    "5  : Lose You To Love Me by Selena Gomez\n",
    "6  : Lose You To Love Me by Selena Gomez\n",
    "7  : Flashlight - From \"Pitch Perfect 2\" Soundtrack by Jessie J\n",
    "8  : Starving by Hailee Steinfeld\n",
    "9  : Young & Alive by Bazzi\n",
    "10 : Fake by Lauv\n",
    "```\n",
    "\n",
    "An example of a longer playlist is also shown below:\n",
    "```\n",
    "1  : Please Notice by Christian Leave\n",
    "2  : The Rose Song - From \"High School Musical: The Musical: The Series (Season 2)\" by Olivia Rodrigo\n",
    "3  : Even When/The Best Part - From \"High School Musical: The Musical: The Series (Season 2)\" by Olivia Rodrigo\n",
    "4  : Even When/The Best Part - From \"High School Musical: The Musical: The Series (Season 2)\" by Olivia Rodrigo\n",
    "5  : Lose You To Love Me by Selena Gomez\n",
    "6  : Lose You To Love Me by Selena Gomez\n",
    "7  : Too Good At Goodbyes by Sam Smith\n",
    "8  : Prom Queen by Beach Bunny\n",
    "9  : Vibez by ZAYN\n",
    "10 : Lose You To Love Me by Selena Gomez\n",
    "11 : Bed Peace by Jhené Aiko\n",
    "12 : Starving by Hailee Steinfeld\n",
    "13 : Hot Stuff by Kygo\n",
    "14 : sweetener by Ariana Grande\n",
    "15 : Hot Stuff by Kygo\n",
    "16 : Even When/The Best Part - From \"High School Musical: The Musical: The Series (Season 2)\" by Olivia Rodrigo\n",
    "17 : Bed Peace by Jhené Aiko\n",
    "18 : Spaceman by Nick Jonas\n",
    "19 : Castaways by The Backyardigans\n",
    "20 : One by Ed Sheeran\n",
    "21 : Too Good At Goodbyes by Sam Smith\n",
    "22 : Castaways by The Backyardigans\n",
    "23 : happier by Olivia Rodrigo\n",
    "24 : Too Good At Goodbyes by Sam Smith\n",
    "25 : Funny by Zedd\n",
    "26 : Wondering - From \"High School Musical: The Musical: The Series\" by Olivia Rodrigo\n",
    "27 : The Rose Song - From \"High School Musical: The Musical: The Series (Season 2)\" by Olivia Rodrigo\n",
    "28 : Castaways by The Backyardigans\n",
    "29 : Bed Peace by Jhené Aiko\n",
    "30 : Funny by Zedd\n",
    "```\n",
    "Note, the number of repeated songs. This could be a result of the closeness of many features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by \"sanity-checking\" the model by giving it some easy mood transitions (low to high, high to high). We also proceed by asking the model to generate a playlist with a random number of songs between 10 and 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We load the state from the previously trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(tokenizer=tokenizer, embed_dim=64, hidden_dim=256, \n",
    "              dropout=0.2, lr=1e-3, weight_decay=1e-6, )\n",
    "model.load_state_dict(torch.load('models/baseline-8-3.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "moods = [[['sad', 'lonely'], ['serene', 'relaxed'], ['powerful', 'energetic']],\n",
    "         [['agitated', 'angry'], ['soothed', 'serene', 'relaxed']],\n",
    "         [['irritated', 'nervous'], ['meditative', 'tender', 'melancholic']],\n",
    "         [['joyful', 'dreamy', 'motivated'], ['motivated', 'powerful', 'happy']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mood_states = [json.dumps(tokenizer.moods_to_token(v)) for v in moods]\n",
    "rows = [[v, random.randint(10, 20)] for v in mood_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>moods_states</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[[22, 15], [23, 21], [20, 9]]</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[[1, 3], [25, 23, 21]]</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[[13, 19], [16, 26, 17]]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[[14, 8, 18], [18, 20, 11]]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    moods_states  length\n",
       "0  [[22, 15], [23, 21], [20, 9]]      10\n",
       "1         [[1, 3], [25, 23, 21]]      15\n",
       "2       [[13, 19], [16, 26, 17]]      12\n",
       "3    [[14, 8, 18], [18, 20, 11]]      14"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtf = pd.DataFrame(rows, columns=['moods_states', 'length'])\n",
    "dtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TestDataset(dtf.copy(), transform=transform,\n",
    "                 batch_size=1)\n",
    "test_loader = DataLoader(test,\n",
    "                          batch_size=1,\n",
    "                          collate_fn=test_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate the playlist associated with each mood; here, should probably considering on generating the playlist in batches rather than in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1  : Imagine by Jack Johnson\n",
    "2  : Almost Is Never Enough by Ariana Grande\n",
    "3  : Solar Power by Lorde\n",
    "4  : I Will Follow You into the Dark by Death Cab for Cutie\n",
    "5  : Welcome Home, Son by Radical Face\n",
    "6  : She Is Love by Parachute\n",
    "7  : Welcome Home, Son by Radical Face\n",
    "8  : Flashlight - From \"Pitch Perfect 2\" Soundtrack by Jessie J\n",
    "9  : sweetener by Ariana Grande\n",
    "10 : Flashlight - From \"Pitch Perfect 2\" Soundtrack by Jessie J\n",
    "11 : Little Lion Man by Mumford & Sons\n",
    "12 : 1999 WILDFIRE by BROCKHAMPTON\n",
    "13 : Old Friends by Pinegrove\n",
    "14 : Lose You To Love Me by Selena Gomez\n",
    "15 : Little Lion Man by Mumford & Sons\n",
    "16 : Too Good At Goodbyes by Sam Smith\n",
    "17 : Lose You To Love Me by Selena Gomez\n",
    "18 : I Will Follow You into the Dark by Death Cab for Cutie\n",
    "19 : Old Pine by Ben Howard\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "results = []\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    results.append(model.test_step(batch, batch_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each song and each respective playlist into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = [v.detach().numpy().tolist() for v in results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Names of Songs\n",
    "Now we query the Spotify API using the `get_recommendations` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_TOKEN = open('api').readline()\n",
    "SOAR_ID = \"etoj1vywg8pvjmuxgovg6l9kb\"\n",
    "headers = {\"Authorization\": f\"Bearer {API_TOKEN}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendation(feature, artists, tracks, genres,\n",
    "                       playlist, order, lock, beam_width):\n",
    "    url = \"https://api.spotify.com/v1/recommendations\"\n",
    "    f = [\"acousticness\", \"danceability\", \"energy\", \"instrumentalness\", \n",
    "         \"key\", \"liveness\", \"loudness\", \"mode\", \"speechiness\", \"tempo\", \"valence\"]\n",
    "    params = {f'target_{f[i]}': v\n",
    "              for i, v in enumerate(feature)}\n",
    "    # round integer fields -> \n",
    "    params['target_key'] = round(params['target_key'])\n",
    "    params['target_mode'] = round(params['target_mode'])\n",
    "    params['target_tempo'] = round(params['target_tempo'])\n",
    "    # get seed artists, tracks, and genres\n",
    "    params['seed_artists'] = artists\n",
    "    params['seed_tracks'] = tracks\n",
    "    params['seed_genres'] = genres\n",
    "    params['limit'] = 10\n",
    "    # get id of recommended track\n",
    "    reqst = requests.get(url, headers=headers, params=params)\n",
    "    reqst = reqst.json()\n",
    "    track_ids = [reqst['tracks'][i]['id'] for i in range(beam_width)]\n",
    "    # store track id in appropriate index in dict\n",
    "    lock.acquire()\n",
    "    playlist[order] = get_track_name(','.join(track_ids))\n",
    "    lock.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_name(id):\n",
    "    params = {'ids': id}\n",
    "    req = requests.get('https://api.spotify.com/v1/tracks', \n",
    "                       headers=headers, params=params).json()\n",
    "    tracks = []\n",
    "    for i in range(len(req['tracks'])):\n",
    "        tracks.append((req['tracks'][i]['name'],\n",
    "                       req['tracks'][i]['artists'][0]['name']))\n",
    "    return tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  : [('You Belong With Me (Taylor’s Version)', 'Taylor Swift'), ('Welcome Home, Son', 'Radical Face'), ('Arms', 'The Paper Kites'), ('Honey Jars', 'Bryan John Appleby'), ('Imagine', 'Jack Johnson')]\n",
      "2  : [('Almost Is Never Enough', 'Ariana Grande'), ('Goodmorning', 'Bleachers'), ('Canyon Moon', 'Harry Styles'), ('Furr', 'Blitzen Trapper'), ('Hey There Delilah', \"Plain White T's\")]\n",
      "3  : [('Starving', 'Hailee Steinfeld'), ('33 “GOD”', 'Bon Iver'), ('Girl Crush - Recorded at Metropolis Studios, London', 'Harry Styles'), ('Solar Power', 'Lorde'), ('Things That Stop You Dreaming', 'Passenger')]\n",
      "4  : [('Too Good At Goodbyes', 'Sam Smith'), ('Master Of None', 'Beach House'), ('Funny', 'Zedd'), ('I Will Follow You into the Dark', 'Death Cab for Cutie'), ('Wrecking Ball', 'Miley Cyrus')]\n",
      "5  : [('Coming Home', 'Leon Bridges'), ('You Belong With Me (Taylor’s Version)', 'Taylor Swift'), ('Love To Dream', 'Doja Cat'), ('Without Me', 'Halsey'), ('drivers license', 'Olivia Rodrigo')]\n",
      "6  : [('Shimmer (feat. Tyler Ward)', 'Boyce Avenue'), ('You Belong With Me (Taylor’s Version)', 'Taylor Swift'), ('Welcome Home, Son', 'Radical Face'), ('False Confidence', 'Noah Kahan'), ('More Than Words', 'Extreme')]\n",
      "7  : [('She Is Love', 'Parachute'), ('drivers license', 'Olivia Rodrigo'), ('Love To Dream', 'Doja Cat'), ('Sky Walker (feat. Travis Scott)', 'Miguel'), ('Without Me', 'Halsey')]\n",
      "8  : [('Little Lion Man', 'Mumford & Sons'), ('The Scientist', 'Coldplay'), ('Old Pine', 'Ben Howard'), ('Flashlight - From \"Pitch Perfect 2\" Soundtrack', 'Jessie J'), ('Roar (feat. Bea Miller)', 'Boyce Avenue')]\n",
      "9  : [('Things That Stop You Dreaming', 'Passenger'), ('sweetener', 'Ariana Grande'), ('Starving', 'Hailee Steinfeld'), ('Need You Now (feat. Savannah Outen)', 'Boyce Avenue'), ('Lose You To Love Me', 'Selena Gomez')]\n",
      "10 : [('1999 WILDFIRE', 'BROCKHAMPTON'), ('Salad Days', 'Mac DeMarco'), ('In My Blood', 'Shawn Mendes'), ('Little Lion Man', 'Mumford & Sons'), ('Suicide Saturday', 'Hippo Campus')]\n",
      "11 : [('Too Good At Goodbyes', 'Sam Smith'), ('Leave The Door Open', 'Bruno Mars'), ('In My Blood', 'Shawn Mendes'), ('My Shit', 'A Boogie Wit da Hoodie'), ('Little Lion Man', 'Mumford & Sons')]\n",
      "12 : [('1999 WILDFIRE', 'BROCKHAMPTON'), ('Bad Bitch (feat. Ty Dolla $ign)', 'Bebe Rexha'), ('Leave The Door Open', 'Bruno Mars'), ('Suicide Saturday', 'Hippo Campus'), ('Too Good At Goodbyes', 'Sam Smith')]\n",
      "13 : [('Starving', 'Hailee Steinfeld'), ('Without Me', 'Halsey'), ('Candy Paint', 'Post Malone'), ('sweetener', 'Ariana Grande'), ('Fly Away', 'Tones And I')]\n",
      "14 : [('Be Kind (with Halsey)', 'Marshmello'), ('Too Good At Goodbyes', 'Sam Smith'), ('Happier', 'Ed Sheeran'), ('sweetener', 'Ariana Grande'), ('Things That Stop You Dreaming', 'Passenger')]\n",
      "15 : [('Better Together', 'Jack Johnson'), ('In My Blood', 'Shawn Mendes'), ('1999 WILDFIRE', 'BROCKHAMPTON'), ('Leave The Door Open', 'Bruno Mars'), ('Flashlight - From \"Pitch Perfect 2\" Soundtrack', 'Jessie J')]\n",
      "16 : [('Leave The Door Open', 'Bruno Mars'), ('Old Pine', 'Ben Howard'), ('Too Good At Goodbyes', 'Sam Smith'), ('Wrecking Ball', 'Miley Cyrus'), ('Roads', 'Portishead')]\n",
      "17 : [('Crash Into Me', 'Boyce Avenue'), ('Fake', 'Lauv'), ('One', 'Ed Sheeran'), ('Hero Of War', 'Rise Against'), ('Hello My Old Heart', 'The Oh Hellos')]\n",
      "18 : [('Too Good At Goodbyes', 'Sam Smith'), ('Roads', 'Portishead'), ('Master Of None', 'Beach House'), ('I Will Follow You into the Dark', 'Death Cab for Cutie'), ('Funny', 'Zedd')]\n",
      "19 : [('Leave The Door Open', 'Bruno Mars'), ('Roads', 'Portishead'), ('Jesus, Etc.', 'Wilco'), ('Funny', 'Zedd'), ('Wrecking Ball', 'Miley Cyrus')]\n",
      "20 : [('Complicated', \"Olivia O'Brien\"), ('Funny', 'Zedd'), ('Master Of None', 'Beach House'), ('Wrecking Ball', 'Miley Cyrus'), ('Hello My Old Heart', 'The Oh Hellos')]\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    result = result[0]\n",
    "    playlist = {}\n",
    "\n",
    "    # Create threads\n",
    "    artists = '66CXWjxzNUsdJxJ2JdwvnR'  # ariana grande\n",
    "    seed_songs = '463CkQjx2Zk1yXoBuierM9'  # levitating\n",
    "    threads = []\n",
    "    lock = threading.Lock()\n",
    "    for index, song in enumerate(result):\n",
    "        threads.append(threading.Thread(target=get_recommendation,\n",
    "                                        args=(song, artists, seed_songs, \n",
    "                                              'pop,acoustic,indie', playlist,\n",
    "                                              index, lock, 5)))\n",
    "    # Start threads\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "\n",
    "    # Join threads\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    od = collections.OrderedDict(sorted(playlist.items()))\n",
    "    for k, v in od.items():\n",
    "        print(f'{str(k+1).ljust(3)}: {v}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-Processing\n",
    "Now we proceed with implementing some post-processing that will be able to select the optimal playlist from the many Spotify generated suggestions. The loss function used to score our playlist is scored through two factors: uniqueness and sentimental similarity of the lyrics. This objective function being minimized is as follows:\n",
    "$$\n",
    "L(x) = \\text{count}\\cdot\\text{BCE}(\\text{valence}, \\text{sentiment}),\n",
    "$$\n",
    "where BCE represents the binary cross entropy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = torch.nn.BCELoss()\n",
    "\n",
    "def loss(existing_list, song, valence, sentiment):\n",
    "    count = 0\n",
    "    for exists in existing_list:\n",
    "        if exists[0] == song[0]:\n",
    "            count += 1\n",
    "    return count * bce(torch.Tensor([valence]), \n",
    "                       torch.Tensor([sentiment])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(0,\n",
       "              [('Trouble', 'Cage The Elephant'),\n",
       "               ('Hello My Old Heart', 'The Oh Hellos'),\n",
       "               ('Happier', 'Ed Sheeran'),\n",
       "               ('off the table (with The Weeknd)', 'Ariana Grande'),\n",
       "               ('Stay With Me', 'Sam Smith')]),\n",
       "             (1,\n",
       "              [('Happier', 'Ed Sheeran'),\n",
       "               ('Too Good At Goodbyes', 'Sam Smith'),\n",
       "               ('Praying', 'Kesha'),\n",
       "               ('Solar Power', 'Lorde'),\n",
       "               ('Be Kind (with Halsey)', 'Marshmello')]),\n",
       "             (2,\n",
       "              [('One', 'Ed Sheeran'),\n",
       "               ('Lose You To Love Me', 'Selena Gomez'),\n",
       "               ('Too Good At Goodbyes', 'Sam Smith'),\n",
       "               ('Crash Into Me', 'Boyce Avenue'),\n",
       "               (\"I Won't Give Up\", 'Jason Mraz')]),\n",
       "             (3,\n",
       "              [('First Day Of My Life', 'Bright Eyes'),\n",
       "               ('Crash Into Me', 'Boyce Avenue'),\n",
       "               ('33 “GOD”', 'Bon Iver'),\n",
       "               ('Things That Stop You Dreaming', 'Passenger'),\n",
       "               ('Girl Crush - Recorded at Metropolis Studios, London',\n",
       "                'Harry Styles')]),\n",
       "             (4,\n",
       "              [('Crash Into Me', 'Boyce Avenue'),\n",
       "               ('One', 'Ed Sheeran'),\n",
       "               ('First Day Of My Life', 'Bright Eyes'),\n",
       "               ('Girl Crush - Recorded at Metropolis Studios, London',\n",
       "                'Harry Styles'),\n",
       "               ('Solar Power', 'Lorde')]),\n",
       "             (5,\n",
       "              [('One', 'Ed Sheeran'),\n",
       "               ('First Day Of My Life', 'Bright Eyes'),\n",
       "               ('Girl Crush - Recorded at Metropolis Studios, London',\n",
       "                'Harry Styles'),\n",
       "               ('Lose You To Love Me', 'Selena Gomez'),\n",
       "               ('33 “GOD”', 'Bon Iver')]),\n",
       "             (6,\n",
       "              [('I Will Follow You into the Dark', 'Death Cab for Cutie'),\n",
       "               (\"Don't Know Why\", 'Norah Jones'),\n",
       "               ('Too Good At Goodbyes', 'Sam Smith'),\n",
       "               ('With Or Without You (feat. Kina Grannis)', 'Kina Grannis'),\n",
       "               ('Funny', 'Zedd')]),\n",
       "             (7,\n",
       "              [('Roads', 'Portishead'),\n",
       "               ('Wrecking Ball', 'Miley Cyrus'),\n",
       "               ('Complicated', \"Olivia O'Brien\"),\n",
       "               ('Funny', 'Zedd'),\n",
       "               ('From the Dining Table', 'Harry Styles')]),\n",
       "             (8,\n",
       "              [('Crash Into Me', 'Boyce Avenue'),\n",
       "               ('First Day Of My Life', 'Bright Eyes'),\n",
       "               (\"I Won't Give Up\", 'Jason Mraz'),\n",
       "               ('Hero Of War', 'Rise Against'),\n",
       "               ('Solar Power', 'Lorde')]),\n",
       "             (9,\n",
       "              [('29 #Strafford APTS', 'Bon Iver'),\n",
       "               ('Islands', 'The xx'),\n",
       "               ('Nude', 'Radiohead'),\n",
       "               ('Crash Into Me', 'Boyce Avenue'),\n",
       "               ('Roads', 'Portishead')]),\n",
       "             (10,\n",
       "              [('Lose You To Love Me', 'Selena Gomez'),\n",
       "               ('Crash Into Me', 'Boyce Avenue'),\n",
       "               ('One', 'Ed Sheeran'),\n",
       "               ('Starving', 'Hailee Steinfeld'),\n",
       "               ('First Day Of My Life', 'Bright Eyes')]),\n",
       "             (11,\n",
       "              [('Girl Crush - Recorded at Metropolis Studios, London',\n",
       "                'Harry Styles'),\n",
       "               ('One', 'Ed Sheeran'),\n",
       "               ('29 #Strafford APTS', 'Bon Iver'),\n",
       "               ('Starving', 'Hailee Steinfeld'),\n",
       "               ('First Day Of My Life', 'Bright Eyes')]),\n",
       "             (12,\n",
       "              [('Girl Crush - Recorded at Metropolis Studios, London',\n",
       "                'Harry Styles'),\n",
       "               ('Be Kind (with Halsey)', 'Marshmello'),\n",
       "               ('Lose You To Love Me', 'Selena Gomez'),\n",
       "               ('Crash Into Me', 'Boyce Avenue'),\n",
       "               ('Fake', 'Lauv')]),\n",
       "             (13,\n",
       "              [('First Day Of My Life', 'Bright Eyes'),\n",
       "               ('Crash Into Me', 'Boyce Avenue'),\n",
       "               ('Girl Crush - Recorded at Metropolis Studios, London',\n",
       "                'Harry Styles'),\n",
       "               (\"I Won't Give Up\", 'Jason Mraz'),\n",
       "               ('Lose You To Love Me', 'Selena Gomez')]),\n",
       "             (14,\n",
       "              [('Solar Power', 'Lorde'),\n",
       "               ('29 #Strafford APTS', 'Bon Iver'),\n",
       "               ('Fake', 'Lauv'),\n",
       "               ('One', 'Ed Sheeran'),\n",
       "               ('Crash Into Me', 'Boyce Avenue')]),\n",
       "             (15,\n",
       "              [('Solar Power', 'Lorde'),\n",
       "               ('29 #Strafford APTS', 'Bon Iver'),\n",
       "               ('One', 'Ed Sheeran'),\n",
       "               ('Girl Crush - Recorded at Metropolis Studios, London',\n",
       "                'Harry Styles'),\n",
       "               ('Fake', 'Lauv')]),\n",
       "             (16,\n",
       "              [('Crash Into Me', 'Boyce Avenue'),\n",
       "               ('Girl Crush - Recorded at Metropolis Studios, London',\n",
       "                'Harry Styles'),\n",
       "               ('Lose You To Love Me', 'Selena Gomez'),\n",
       "               ('29 #Strafford APTS', 'Bon Iver'),\n",
       "               ('Solar Power', 'Lorde')]),\n",
       "             (17,\n",
       "              [('Girl Crush - Recorded at Metropolis Studios, London',\n",
       "                'Harry Styles'),\n",
       "               ('Crash Into Me', 'Boyce Avenue'),\n",
       "               ('Lose You To Love Me', 'Selena Gomez'),\n",
       "               ('29 #Strafford APTS', 'Bon Iver'),\n",
       "               ('Complicated', \"Olivia O'Brien\")]),\n",
       "             (18,\n",
       "              [('29 #Strafford APTS', 'Bon Iver'),\n",
       "               ('One', 'Ed Sheeran'),\n",
       "               ('Girl Crush - Recorded at Metropolis Studios, London',\n",
       "                'Harry Styles'),\n",
       "               ('Lose You To Love Me', 'Selena Gomez'),\n",
       "               ('Solar Power', 'Lorde')]),\n",
       "             (19,\n",
       "              [('Solar Power', 'Lorde'),\n",
       "               ('Fake', 'Lauv'),\n",
       "               ('Be Kind (with Halsey)', 'Marshmello'),\n",
       "               ('One', 'Ed Sheeran'),\n",
       "               ('29 #Strafford APTS', 'Bon Iver')])])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
